{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check cuda availability\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparams\n",
    "IMG_DIR = 'spectrogram_images/'\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "NUM_CLASSES = 7\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 8\n",
    "L2_LAMBDA = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'Hip':0,\n",
    "              'Pop':1,\n",
    "              'Vocal':2,\n",
    "              'Rhythm':3,\n",
    "              'Reggae':4,\n",
    "              'Rock':5,\n",
    "              'Techno':6,\n",
    "             }\n",
    "\n",
    "one_hot = OneHotEncoder(categories=[range(NUM_CLASSES)])\n",
    "\n",
    "# get working directory\n",
    "cur_dir = os.getcwd()\n",
    "root_dir = os.path.dirname(cur_dir)\n",
    "specto_dir = os.path.join(root_dir, IMG_DIR)\n",
    "all_files = os.listdir(specto_dir)\n",
    "\n",
    "# Get class weights\n",
    "label_array = []\n",
    "for file_ in all_files:\n",
    "    vals = file_[:-4].split('_')\n",
    "    label_array.append(label_dict[vals[1]])\n",
    "    \n",
    "cl_weight = compute_class_weight(class_weight = 'balanced', \n",
    "                                 classes = np.unique(label_array), \n",
    "                                 y = label_array)\n",
    "cl_weight = torch.tensor(cl_weight, dtype=torch.float32)\n",
    "\n",
    "# Train-val-test split of files\n",
    "train_files, test_files, train_labels, test_labels = train_test_split(all_files, \n",
    "                                                                      label_array,\n",
    "                                                                      random_state = 10, \n",
    "                                                                      test_size = 0.1\n",
    "                                                                     )\n",
    "\n",
    "# Among the test files, keep half for validation\n",
    "val_files, test_files, val_labels, test_labels = train_test_split(test_files, test_labels,\n",
    "                                                                  random_state = 10, \n",
    "                                                                  test_size = 0.5\n",
    "                                                                 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, files, specto_dir, label_dict, IMG_WIDTH, IMG_HEIGHT):\n",
    "        self.files = files\n",
    "        self.specto_dir = specto_dir\n",
    "        self.label_dict = label_dict\n",
    "        self.one_hot = one_hot\n",
    "        self.IMG_WIDTH = IMG_WIDTH\n",
    "        self.IMG_HEIGHT = IMG_HEIGHT\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_ = self.files[idx]\n",
    "        im = Image.open(self.specto_dir + file_)\n",
    "        im = im.resize((self.IMG_WIDTH, self.IMG_HEIGHT), Image.Resampling.LANCZOS)\n",
    "        spectogram = np.array(im) / 255.0\n",
    "        \n",
    "        label = file_[:-4].split('_')\n",
    "        label_array = np.array([self.label_dict[label[1]]])\n",
    "        label_array = label_array.reshape(1, -1)\n",
    "        label_array = one_hot.fit_transform(label_array).toarray()\n",
    "\n",
    "        return spectogram, np.array(label_array[0])\n",
    "    \n",
    "# Initialize datasets\n",
    "train_dataset = CustomDataset(train_files, specto_dir, label_dict, IMG_WIDTH, IMG_HEIGHT)\n",
    "val_dataset = CustomDataset(val_files, specto_dir, label_dict, IMG_WIDTH, IMG_HEIGHT)\n",
    "test_dataset = CustomDataset(test_files, specto_dir, label_dict, IMG_WIDTH, IMG_HEIGHT)\n",
    "\n",
    "# Initialize DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define pretrained ViT model from Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = models.vit_l_16(weights='DEFAULT')\n",
    "in_features = conv_base.heads[0].in_features\n",
    "conv_base.heads = torch.nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tuning, allow resnet pretrained weights to be trainable\n",
    "for param in conv_base.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "  conv_base,\n",
    "  nn.Flatten(),\n",
    "  nn.Linear(in_features, 512),\n",
    "  nn.Dropout(p=0.3),\n",
    "  nn.ReLU(),\n",
    "  nn.Linear(512, NUM_CLASSES),\n",
    "  nn.Softmax(dim=-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set training optimizer, loss, and metrics\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=L2_LAMBDA)\n",
    "loss_function = nn.CrossEntropyLoss(weight=cl_weight)\n",
    "\n",
    "def categorical_accuracy(output, target):\n",
    "    predicted = torch.argmax(output, dim=-1)\n",
    "    labels = torch.argmax(target, dim=-1)\n",
    "    correct = (predicted == labels).float()\n",
    "    return correct.sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/3962 [00:22<25:00:05, 22.72s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, targets)  \u001b[38;5;66;03m# Calculate the loss\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[0;32m     30\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Alex\\miniconda3\\envs\\csci1470\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Alex\\miniconda3\\envs\\csci1470\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Calculate number of steps per epoch\n",
    "STEPS_PER_EPOCH = len(train_files) // BATCH_SIZE\n",
    "VAL_STEPS = len(val_files) // BATCH_SIZE\n",
    "\n",
    "# Initialize lists to store training and validation losses and accuracies\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Training\n",
    "    model.train()  # Set the model to train mode\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for batch_idx, (inputs, targets) in tqdm(enumerate(train_loader), total=STEPS_PER_EPOCH):\n",
    "        # Permute the inputs to [N, C, H, W] from [N, H, W, C]\n",
    "        inputs = inputs.permute(0, 3, 1, 2)\n",
    "        inputs = inputs.to(device, dtype=torch.float32)\n",
    "        targets = targets.to(device, dtype=torch.float32)\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = loss_function(outputs, targets)  # Calculate the loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        train_loss += loss.item()\n",
    "        # aggregate total number correct\n",
    "        correct_train += categorical_accuracy(outputs, targets)\n",
    "        total_train += targets.size(0)\n",
    "\n",
    "    # Calculate average training loss and accuracy\n",
    "    avg_train_loss = train_loss / STEPS_PER_EPOCH\n",
    "    train_accuracy = 100. * correct_train / total_train\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in tqdm(enumerate(val_loader), total=VAL_STEPS):\n",
    "            # Permute the inputs to [N, C, H, W] from [N, H, W, C]\n",
    "            inputs = inputs.permute(0, 3, 1, 2)\n",
    "            inputs = inputs.to(device, dtype=torch.float32)\n",
    "            targets = targets.to(device, dtype=torch.float32)\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = loss_function(outputs, targets)  # Calculate the loss\n",
    "            val_loss += loss.item()\n",
    "            # _, predicted = outputs.max(1)\n",
    "            total_val += targets.size(0)\n",
    "            correct_val += categorical_accuracy(outputs, targets)\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    avg_val_loss = val_loss / VAL_STEPS\n",
    "    val_accuracy = 100. * correct_val / total_val\n",
    "\n",
    "    # Print training and validation metrics\n",
    "    print(f'Epoch {epoch + 1}/{NUM_EPOCHS}, '\n",
    "          f'Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%, '\n",
    "          f'Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    # Save the model checkpoint\n",
    "    cur_dir = os.getcwd()\n",
    "    root_dir = os.path.dirname(cur_dir)\n",
    "    ckpt_dir = os.path.join(root_dir, f'saved_models/fine_tuning_epoch_{epoch + 1}_{val_accuracy:.4f}.pt')\n",
    "    torch.save(model.state_dict(), ckpt_dir)\n",
    "\n",
    "    # Append metrics to lists for plotting later if needed\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scores on train and validation sets\n",
    "cur_dir = os.getcwd()\n",
    "root_dir = os.path.dirname(cur_dir)\n",
    "pkl_dir = os.path.join(root_dir, 'pickle_files/fine_tuning_vit_l_16_pytorch_history.pkl')\n",
    "\n",
    "history = {\n",
    "    'train_loss': train_losses,\n",
    "    'train_accuracy': train_accuracies,\n",
    "    'val_loss': val_losses,\n",
    "    'val_accuracy': val_accuracies,\n",
    "}\n",
    "\n",
    "with open(pkl_dir, 'wb') as f:\n",
    "    pickle.dump(history, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci1470",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
