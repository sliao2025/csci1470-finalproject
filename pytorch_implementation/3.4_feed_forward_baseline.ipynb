{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check cuda availability\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMG_DIR = 'spectrogram_images/fma_spectrogram_images/'\n",
    "IMG_HEIGHT = 216\n",
    "IMG_WIDTH = 216\n",
    "NUM_CLASSES = 16\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "L2_LAMBDA = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_dict = {'Hip':0,\n",
    "              'Pop':1,\n",
    "              'Vocal':2,\n",
    "              'Rhythm':3,\n",
    "              'Reggae':4,\n",
    "              'Rock':5,\n",
    "              'Techno':6,\n",
    "             }\n",
    "\n",
    "one_hot = OneHotEncoder(categories=[range(NUM_CLASSES)])\n",
    "\n",
    "# get working directory\n",
    "cur_dir = os.getcwd()\n",
    "root_dir = os.path.dirname(cur_dir)\n",
    "specto_dir = os.path.join(root_dir, IMG_DIR)\n",
    "all_files = os.listdir(specto_dir)\n",
    "\n",
    "# Get class weights\n",
    "label_array = []\n",
    "for file_ in all_files:\n",
    "    vals = file_[:-4].split('_')\n",
    "    label_array.append(label_dict[vals[1]])\n",
    "    \n",
    "cl_weight = compute_class_weight(class_weight = 'balanced', \n",
    "                                 classes = np.unique(label_array), \n",
    "                                 y = label_array)\n",
    "cl_weight = torch.tensor(cl_weight, dtype=torch.float32)\n",
    "\n",
    "# Train-val-test split of files\n",
    "train_files, test_files, train_labels, test_labels = train_test_split(all_files, \n",
    "                                                                      label_array,\n",
    "                                                                      random_state = 10, \n",
    "                                                                      test_size = 0.1\n",
    "                                                                     )\n",
    "\n",
    "# Among the test files, keep half for validation\n",
    "val_files, test_files, val_labels, test_labels = train_test_split(test_files, test_labels,\n",
    "                                                                  random_state = 10, \n",
    "                                                                  test_size = 0.5\n",
    "                                                                 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, files, specto_dir, label_dict, IMG_WIDTH, IMG_HEIGHT):\n",
    "        self.files = files\n",
    "        self.specto_dir = specto_dir\n",
    "        self.label_dict = label_dict\n",
    "        self.one_hot = one_hot\n",
    "        self.IMG_WIDTH = IMG_WIDTH\n",
    "        self.IMG_HEIGHT = IMG_HEIGHT\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_ = self.files[idx]\n",
    "        im = Image.open(self.specto_dir + file_)\n",
    "        im = im.resize((self.IMG_WIDTH, self.IMG_HEIGHT), Image.Resampling.LANCZOS)\n",
    "        spectogram = np.array(im) / 255.0\n",
    "        \n",
    "        label = file_[:-4].split('_')\n",
    "        label_array = np.array([self.label_dict[label[1]]])\n",
    "        label_array = label_array.reshape(1, -1)\n",
    "        label_array = one_hot.fit_transform(label_array).toarray()\n",
    "\n",
    "        return spectogram, np.array(label_array[0])\n",
    "    \n",
    "# Initialize datasets\n",
    "train_dataset = CustomDataset(train_files, specto_dir, label_dict, IMG_WIDTH, IMG_HEIGHT)\n",
    "val_dataset = CustomDataset(val_files, specto_dir, label_dict, IMG_WIDTH, IMG_HEIGHT)\n",
    "test_dataset = CustomDataset(test_files, specto_dir, label_dict, IMG_WIDTH, IMG_HEIGHT)\n",
    "\n",
    "# Initialize DataLoader instances\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Feedforward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "  nn.Flatten(),\n",
    "  nn.Linear(IMG_HEIGHT * IMG_WIDTH * 3, 512),\n",
    "  nn.Dropout(p=0.3),\n",
    "  nn.ReLU(),\n",
    "  nn.Linear(512, 32),\n",
    "  nn.Dropout(p=0.3),\n",
    "  nn.ReLU(),\n",
    "  nn.Linear(32, NUM_CLASSES),\n",
    "  nn.Softmax(dim=-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Flatten: 1-1                           --\n",
      "├─Linear: 1-2                            71,664,128\n",
      "├─Dropout: 1-3                           --\n",
      "├─ReLU: 1-4                              --\n",
      "├─Linear: 1-5                            16,416\n",
      "├─Dropout: 1-6                           --\n",
      "├─ReLU: 1-7                              --\n",
      "├─Linear: 1-8                            231\n",
      "├─Softmax: 1-9                           --\n",
      "=================================================================\n",
      "Total params: 71,680,775\n",
      "Trainable params: 71,680,775\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─Flatten: 1-1                           --\n",
       "├─Linear: 1-2                            71,664,128\n",
       "├─Dropout: 1-3                           --\n",
       "├─ReLU: 1-4                              --\n",
       "├─Linear: 1-5                            16,416\n",
       "├─Dropout: 1-6                           --\n",
       "├─ReLU: 1-7                              --\n",
       "├─Linear: 1-8                            231\n",
       "├─Softmax: 1-9                           --\n",
       "=================================================================\n",
       "Total params: 71,680,775\n",
       "Trainable params: 71,680,775\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Feedforward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set training optimizer, loss, and metrics\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=L2_LAMBDA)\n",
    "cl_weight = cl_weight.to(device)\n",
    "loss_function = nn.CrossEntropyLoss(weight=cl_weight)\n",
    "\n",
    "\n",
    "def categorical_accuracy(output, target):\n",
    "    predicted = torch.argmax(output, dim=-1)\n",
    "    labels = torch.argmax(target, dim=-1)\n",
    "    correct = (predicted == labels).float()\n",
    "    return correct.sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate number of steps per epoch\n",
    "STEPS_PER_EPOCH = len(train_files) // BATCH_SIZE\n",
    "VAL_STEPS = len(val_files) // BATCH_SIZE\n",
    "\n",
    "# Initialize lists to store training and validation losses and accuracies\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Training\n",
    "    model.train()  # Set the model to train mode\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for batch_idx, (inputs, targets) in tqdm(enumerate(train_loader), total=STEPS_PER_EPOCH):\n",
    "        # Permute the inputs to [N, C, H, W] from [N, H, W, C]\n",
    "        inputs = inputs.permute(0, 3, 1, 2)\n",
    "        inputs = inputs.to(device, dtype=torch.float32)\n",
    "        targets = targets.to(device, dtype=torch.float32)\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        ouputs = outputs.to(device)\n",
    "        loss = loss_function(outputs, targets)  # Calculate the loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        train_loss += loss.item()\n",
    "        # aggregate total number correct\n",
    "        correct_train += categorical_accuracy(outputs, targets)\n",
    "        total_train += targets.size(0)\n",
    "\n",
    "    # Calculate average training loss and accuracy\n",
    "    avg_train_loss = train_loss / STEPS_PER_EPOCH\n",
    "    train_accuracy = 100. * correct_train / total_train\n",
    "\n",
    "    # Validation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in tqdm(enumerate(val_loader), total=VAL_STEPS):\n",
    "            # Permute the inputs to [N, C, H, W] from [N, H, W, C]\n",
    "            inputs = inputs.permute(0, 3, 1, 2)\n",
    "            inputs = inputs.to(device, dtype=torch.float32)\n",
    "            targets = targets.to(device, dtype=torch.float32)\n",
    "            loss = loss_function(outputs, targets)  # Calculate the loss\n",
    "            val_loss += loss.item()\n",
    "            # _, predicted = outputs.max(1)\n",
    "            total_val += targets.size(0)\n",
    "            correct_val += categorical_accuracy(outputs, targets)\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    avg_val_loss = val_loss / VAL_STEPS\n",
    "    val_accuracy = 100. * correct_val / total_val\n",
    "\n",
    "    # Print training and validation metrics\n",
    "    print(f'Epoch {epoch + 1}/{NUM_EPOCHS}, '\n",
    "          f'Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%, '\n",
    "          f'Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    # Save the model checkpoint\n",
    "    cur_dir = os.getcwd()\n",
    "    root_dir = os.path.dirname(cur_dir)\n",
    "    ckpt_dir = os.path.join(root_dir, f'saved_models/ff_epoch_{epoch + 1}_{val_accuracy:.4f}.pt')\n",
    "    torch.save(model.state_dict(), ckpt_dir)\n",
    "\n",
    "    # Append metrics to lists for plotting later if needed\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save scores on train and validation sets\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "root_dir = os.path.dirname(cur_dir)\n",
    "pkl_dir = os.path.join(root_dir, 'pickle_files\\\\ff_pytorch_history.pkl')\n",
    "\n",
    "history = {\n",
    "    'train_loss': train_losses,\n",
    "    'train_accuracy': train_accuracies,\n",
    "    'val_loss': val_losses,\n",
    "    'val_accuracy': val_accuracies,\n",
    "}\n",
    "\n",
    "with open(pkl_dir, 'wb') as f:\n",
    "    pickle.dump(history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test files\n",
    "pkl_dir = os.path.join(root_dir, 'pickle_files\\\\ff_pytorch_test_files.pkl')\n",
    "with open(pkl_dir, 'wb') as f:\n",
    "    pickle.dump(test_files, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
